# """
# –°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞–∫—É–±:
#   ‚Ä¢ MODIS MOD13Q1  NDVI, EVI
#   ‚Ä¢ SMAP L4        soil moisture (surface + root)
#   ‚Ä¢ ERA5-Land      t2m, vpd, swrad
#   + spi3 (–∏–∑ chirps_spi.nc)

# –í—ã–≤–æ–¥: data/processed/agro_cube.zarr
# –ó–∞–ø—É—Å–∫–∞—Ç—å –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞:
#     python -m src.data_pipeline.prepare_cube
# """

# import xarray as xr, rioxarray as rio, numpy as np, os, tempfile, shutil
# from pathlib import Path
# import warnings, datetime as dt

# RAW   = Path("data/raw")
# PROC  = Path("data/processed")
# SPI_NC = RAW / "chirps_spi.nc"
# ZARR  = PROC / "agro_cube.zarr"

# # ---- ROI –≥—Ä–∞–Ω–∏—Ü—ã (—Ç–æ—Ç –∂–µ —Å–ª–æ–≤–∞—Ä—å) ----
# REGIONS = {
#     "us_plains":  (35, 48, -104, -90),
#     "br_cerrado": (-20, -6, -62,  -46),
#     "in_ganga":   (21, 31,  73,   90),
# }
# LAT_MIN = min(r[0] for r in REGIONS.values())
# LAT_MAX = max(r[1] for r in REGIONS.values())
# LON_MIN = min(r[2] for r in REGIONS.values())
# LON_MAX = max(r[3] for r in REGIONS.values())

# def clip_roi(ds: xr.Dataset) -> xr.Dataset:
#     lat_asc = float(ds.latitude[1]) > float(ds.latitude[0])
#     lat_slice = slice(LAT_MIN, LAT_MAX) if lat_asc else slice(LAT_MAX, LAT_MIN)
#     return ds.sel(latitude=lat_slice, longitude=slice(LON_MIN, LON_MAX))

# def load_spi() -> xr.Dataset:
#     ds = xr.open_dataset(SPI_NC)
#     return ds.rename(time="time")   # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –µ–¥–∏–Ω–æ–µ –∏–º—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã

# def load_modis() -> xr.Dataset:
#     """
#     –°–∫–∞—á–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Ç–∞–π–ª—ã MOD13Q1 (NDVI) —á–µ—Ä–µ–∑ HTTPS LP-DAAC.
#     –ë–µ–∑ earthaccess, –±–µ–∑ S3.  –†–∞–±–æ—Ç–∞–µ—Ç ~15-20 –º–∏–Ω –Ω–∞ 300 HDF.
#     """
#     import datetime as dt, itertools, tempfile, rioxarray as rio, xarray as xr
#     from pathlib import Path, PurePosixPath
#     import requests, tqdm, warnings, os

#     TILES = ["h09v04","h10v04","h11v04", "h13v10","h14v10",
#              "h25v07","h26v07"]
#     start = dt.date(2003,1,1)
#     end   = dt.date(2024,12,31)

#     # –∫–∞–∂–¥—ã–µ 16 –¥–Ω–µ–π (MOD13Q1)
#     dates = [start + dt.timedelta(days=i)
#              for i in range(0, (end-start).days+1, 16)]

#     tmp_dir = Path(tempfile.mkdtemp())
#     sess = requests.Session()
#     sess.auth = (os.getenv("ED_USERNAME"), os.getenv("ED_PASSWORD"))  # –∏–ª–∏ ~/.netrc

#     hdf_paths = []

#     for d in tqdm.tqdm(dates, desc="MODIS days"):
#         folder = f"{d:%Y.%m.%d}"
#         doy    = d.timetuple().tm_yday
#         for tile in TILES:
#             # –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ö–≤–æ—Å—Ç–∞ –ø–æ—Å–ª–µ .061 ‚Äî –Ω–∞–π–¥—ë–º –∑–∞–ø—Ä–æ—Å–æ–º –Ω–∞ HTML-–ª–∏—Å—Ç–∏–Ω–≥
#             prefix = f"MOD13Q1.A{d.year}{doy:03d}.{tile}.061"
#             url_list = (f"https://e4ftl01.cr.usgs.gov/MOLT/MOD13Q1.061/"
#                         f"{folder}/")
#             try:
#                 r = sess.get(url_list, timeout=30)
#                 r.raise_for_status()
#                 fname = next(l.split('">')[0]
#                              for l in r.text.split()
#                              if l.startswith(prefix) and l.endswith(".hdf"))
#                 url = PurePosixPath(url_list) / fname
#                 out = tmp_dir / fname
#                 if not out.exists():
#                     with sess.get(str(url), stream=True, timeout=120) as resp:
#                         resp.raise_for_status()
#                         with open(out, "wb") as f:
#                             for chunk in resp.iter_content(1024*1024):
#                                 f.write(chunk)
#                 hdf_paths.append(out)
#             except Exception:
#                 # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–ª–∏ —Ä–≤–∞–Ω—ã–µ —Ñ–∞–π–ª—ã
#                 continue

#     # —á–∏—Ç–∞–µ–º NDVI —Å–ª–æ–π
#     stacks = []
#     for f in hdf_paths:
#         sds = (f"HDF4_EOS:EOS_GRID:{f}:"
#                "MODIS_Grid_16DAY_250m_500m_VI:1 km VI NDVI")
#         da  = rio.open_rasterio(sds, masked=True).squeeze()
#         date_code = f.name.split('.')[1]   # A2003353
#         dtc = dt.datetime.strptime(date_code[1:], "%Y%j")
#         da = da.assign_coords({"time": dtc}).expand_dims("time")
#         stacks.append(da)

#     if not stacks:
#         raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ NDVI-—Ñ–∞–π–ª–∞")

#     ndvi = xr.concat(stacks, dim="time").rename({"y":"latitude","x":"longitude"})
#     ndvi = clip_roi(ndvi).resample(time="1M").mean(keep_attrs=True)
#     ndvi.name = "ndvi"
#     warnings.filterwarnings("ignore")
#     return ndvi.to_dataset()

# def load_smap() -> xr.Dataset:
#     url = "https://smappub.jpl.nasa.gov/data/smap_level4/spl4smgp/..."  # —É—Ä–µ–∑–∞–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä
#     sm = xr.open_dataset(url, chunks={"time":31})
#     sm = clip_roi(sm)
#     sm = sm.resample(time="1M").mean()
#     sm = sm[["sm_surface","sm_rootzone"]]
#     return sm

# def load_era5() -> xr.Dataset:
#     import cdsapi, pandas as pd
#     c = cdsapi.Client()
#     out = RAW / "era5_land.nc"
#     if not out.exists():
#         c.retrieve(
#             "reanalysis-era5-land",
#             {"variable":["2m_temperature","surface_net_solar_radiation","vpd"],
#              "year":[str(y) for y in range(2003,2025)],
#              "month":[f"{m:02d}" for m in range(1,13)],
#              "day":"01","time":"00:00",
#              "area":[LAT_MAX, LON_MIN, LAT_MIN, LON_MAX],
#              "format":"netcdf"}, str(out))
#     era = xr.open_dataset(out)
#     era = era.resample(time="1M").mean()
#     era = era.rename({"msdwlwrf":"srad"})
#     return era

# def main():
#     warnings.filterwarnings("ignore")
#     spi  = load_spi()
#     ndvi = load_modis()
#     sm   = load_smap()
#     era  = load_era5()

#     cube = xr.merge([spi, ndvi, sm, era], compat="override")
#     PROC.mkdir(parents=True, exist_ok=True)
#     cube.to_zarr(ZARR, mode="w")
#     print(f"‚úÖ  Saved ‚Üí {ZARR}")

# if __name__ == "__main__":
#     main()

from pathlib import Path
import xarray as xr

RAW = Path("data/raw/chirps_spi.nc")
PROC = Path("data/processed")
ZARR = PROC / "agro_cube.zarr"

def main():
    print("üì¶ –ó–∞–≥—Ä—É–∂–∞—é SPI —Ñ–∞–π–ª‚Ä¶")
    ds = xr.open_dataset(RAW)

    print("üì¶ –ü—Ä–æ–≤–µ—Ä—è—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É‚Ä¶")
    assert "spi3" in ds.data_vars, "–ù–µ—Ç spi3 –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ!"
    assert "time" in ds.dims, "–ù–µ—Ç –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏!"

    print("üì¶ –°–æ—Ö—Ä–∞–Ω—è—é –≤ Zarr‚Ä¶")
    PROC.mkdir(parents=True, exist_ok=True)
    ds.to_zarr(ZARR, mode="w")

    print(f"‚úÖ Saved ‚Üí {ZARR}")

if __name__ == "__main__":
    main()