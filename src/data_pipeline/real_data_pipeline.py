"""
–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Å—É—Ö–∏
–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
- CHIRPS: –æ—Å–∞–¥–∫–∏ (—Ä–µ–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
- ERA5-Land: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –≤–ª–∞–∂–Ω–æ—Å—Ç—å –ø–æ—á–≤—ã, –∏—Å–ø–∞—Ä–µ–Ω–∏–µ 
- MODIS: NDVI (—á–µ—Ä–µ–∑ NASA LAADS DAAC)
- –†–æ—Å—Å–∏–π—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: –†–æ—Å–≥–∏–¥—Ä–æ–º–µ—Ç + –í–ù–ò–ò–°–•–ú

–¢—Ä–µ–±—É–µ—Ç: 
- NASA Earthdata –∞–∫–∫–∞—É–Ω—Ç (.netrc —Ñ–∞–π–ª)
- CDS API –∫–ª—é—á –¥–ª—è ERA5
- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: API –∫–ª—é—á–∏ –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

–ó–∞–ø—É—Å–∫: python -m src.data_pipeline.real_data_pipeline
"""

import os
import shutil
import tempfile
import requests
import warnings
from pathlib import Path
from typing import Dict, Tuple, List, Optional
import datetime as dt
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

import numpy as np
import xarray as xr
import pandas as pd
from scipy.stats import gamma, norm
import rioxarray as rio
from netrc import netrc
import h5py

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
YEARS = range(2003, 2025)
OUT_DIR = Path("data/raw")
PROC_DIR = Path("data/processed")
ZARR_OUT = PROC_DIR / "real_agro_cube.zarr"

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã (–≤–∫–ª—é—á–∞—è –†–æ—Å—Å–∏—é)
REGIONS = {
    "us_plains": (35, 48, -104, -90),      # –°–®–ê - –í–µ–ª–∏–∫–∏–µ —Ä–∞–≤–Ω–∏–Ω—ã
    "br_cerrado": (-20, -6, -62, -46),     # –ë—Ä–∞–∑–∏–ª–∏—è - –°–µ—Ä—Ä–∞–¥–æ  
    "in_ganga": (21, 31, 73, 90),          # –ò–Ω–¥–∏—è - –ì–∞–Ω–≥
    "ru_steppe": (50, 55, 37, 47),         # –†–æ—Å—Å–∏—è - –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ-–ß–µ—Ä–Ω–æ–∑–µ–º–Ω—ã–π
}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π bbox
LAT_MIN = min(r[0] for r in REGIONS.values()) - 1
LAT_MAX = max(r[1] for r in REGIONS.values()) + 1  
LON_MIN = min(r[2] for r in REGIONS.values()) - 1
LON_MAX = max(r[3] for r in REGIONS.values()) + 1

warnings.filterwarnings("ignore", category=RuntimeWarning)

class CHIRPSDownloader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö CHIRPS"""
    
    BASE_URL = "https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p25"
    
    @staticmethod
    def download_year(year: int, dest_dir: Path) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ CHIRPS –∑–∞ –≥–æ–¥"""
        url = f"{CHIRPSDownloader.BASE_URL}/chirps-v2.0.{year}.days_p25.nc"
        dest_file = dest_dir / f"chirps_{year}.nc"
        
        if dest_file.exists():
            print(f"üìÅ CHIRPS {year} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return True
            
        try:
            print(f"‚¨á –ó–∞–≥—Ä—É–∑–∫–∞ CHIRPS {year}...", end="", flush=True)
            response = requests.get(url, stream=True, timeout=600)
            response.raise_for_status()
            
            with open(dest_file, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(" ‚úÖ")
            return True
            
        except Exception as e:
            print(f" ‚ùå –û—à–∏–±–∫–∞: {e}")
            if dest_file.exists():
                dest_file.unlink()
            return False
    
    @staticmethod
    def process_chirps(dest_dir: Path) -> xr.Dataset:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ CHIRPS"""
        print("üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö CHIRPS...")
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(CHIRPSDownloader.download_year, year, dest_dir)
                for year in YEARS
            ]
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∑–æ–∫
            for future in as_completed(futures):
                future.result()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        datasets = []
        for year in YEARS:
            file_path = dest_dir / f"chirps_{year}.nc"
            if file_path.exists():
                try:
                    ds = xr.open_dataset(file_path)
                    datasets.append(ds)
                except Exception as e:
                    print(f"‚ö† –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {year}: {e}")
        
        if not datasets:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ CHIRPS")
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        combined = xr.concat(datasets, dim="time")
        
        # –û–±—Ä–µ–∑–∫–∞ –ø–æ –æ–±–ª–∞—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–µ—Å–∞
        lat_slice = slice(LAT_MIN, LAT_MAX)
        if combined.latitude[0] > combined.latitude[-1]:  # –£–±—ã–≤–∞—é—â–∏–µ —à–∏—Ä–æ—Ç—ã
            lat_slice = slice(LAT_MAX, LAT_MIN)
            
        combined = combined.sel(
            latitude=lat_slice,
            longitude=slice(LON_MIN, LON_MAX)
        )
        
        # –ú–µ—Å—è—á–Ω—ã–µ —Å—É–º–º—ã –æ—Å–∞–¥–∫–æ–≤
        monthly = combined.resample(time="1M").sum()
        
        print(f"‚úÖ CHIRPS: {monthly.dims} –∑–∞ {len(datasets)} –ª–µ—Ç")
        return monthly

class ERA5Downloader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö ERA5-Land"""
    
    @staticmethod
    def setup_cds_api() -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ CDS API"""
        cds_rc = Path.home() / ".cdsapirc"
        if not cds_rc.exists():
            print("‚ö† –ù–µ –Ω–∞–π–¥–µ–Ω ~/.cdsapirc —Ñ–∞–π–ª")
            print("üìù –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º:")
            print("url: https://cds.climate.copernicus.eu/api/v2")
            print("key: UID:API-KEY")
            print("–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –Ω–∞: https://cds.climate.copernicus.eu/api-how-to")
            return False
        return True
    
    @staticmethod
    def download_era5_land(dest_dir: Path) -> xr.Dataset:
        """–ó–∞–≥—Ä—É–∑–∫–∞ ERA5-Land –¥–∞–Ω–Ω—ã—Ö"""
        if not ERA5Downloader.setup_cds_api():
            raise RuntimeError("CDS API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        try:
            import cdsapi
        except ImportError:
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install cdsapi")
        
        era5_file = dest_dir / "era5_land_complete.nc"
        
        if era5_file.exists():
            print("üìÅ ERA5-Land —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return xr.open_dataset(era5_file)
        
        print("‚¨á –ó–∞–≥—Ä—É–∑–∫–∞ ERA5-Land –¥–∞–Ω–Ω—ã—Ö...")
        client = cdsapi.Client()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ –≥–æ–¥–∞–º (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–æ–ª—å—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
        yearly_files = []
        for year in YEARS:
            year_file = dest_dir / f"era5_land_{year}.nc"
            if not year_file.exists():
                print(f"  üìÖ –ó–∞–≥—Ä—É–∑–∫–∞ {year}...")
                try:
                    client.retrieve(
                        'reanalysis-era5-land',
                        {
                            'variable': [
                                '2m_temperature',
                                'total_precipitation', 
                                'potential_evaporation',
                                'volumetric_soil_water_layer_1',
                                'volumetric_soil_water_layer_2',
                                'soil_temperature_level_1',
                            ],
                            'year': str(year),
                            'month': [f'{m:02d}' for m in range(1, 13)],
                            'day': [f'{d:02d}' for d in range(1, 32)],
                            'time': ['00:00', '06:00', '12:00', '18:00'],
                            'area': [LAT_MAX, LON_MIN, LAT_MIN, LON_MAX],  # N, W, S, E
                            'format': 'netcdf',
                        },
                        str(year_file)
                    )
                    yearly_files.append(year_file)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {year}: {e}")
            else:
                yearly_files.append(year_file)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≥–æ–¥–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        if yearly_files:
            datasets = [xr.open_dataset(f) for f in yearly_files]
            combined = xr.concat(datasets, dim="time")
            
            # –ú–µ—Å—è—á–Ω—ã–µ —Å—Ä–µ–¥–Ω–∏–µ
            monthly = combined.resample(time="1M").mean()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            monthly.to_netcdf(era5_file)
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            for f in yearly_files:
                f.unlink()
            
            print(f"‚úÖ ERA5-Land: {monthly.dims}")
            return monthly
        else:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ERA5-Land –¥–∞–Ω–Ω—ã–µ")

class MODISDownloader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö MODIS NDVI"""
    
    @staticmethod
    def setup_earthdata_auth() -> Tuple[str, str]:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ NASA Earthdata"""
        netrc_file = Path.home() / ".netrc"
        
        if netrc_file.exists():
            try:
                auth_info = netrc()
                login, account, password = auth_info.authenticators("urs.earthdata.nasa.gov")
                return login, password
            except:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        username = os.getenv("EARTHDATA_USERNAME")
        password = os.getenv("EARTHDATA_PASSWORD")
        
        if username and password:
            return username, password
            
        print("‚ö† NASA Earthdata –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
        print("üìù –°–æ–∑–¥–∞–π—Ç–µ ~/.netrc —Ñ–∞–π–ª:")
        print("machine urs.earthdata.nasa.gov")
        print("login YOUR_USERNAME") 
        print("password YOUR_PASSWORD")
        print("\n–ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
        print("export EARTHDATA_USERNAME=your_username")
        print("export EARTHDATA_PASSWORD=your_password")
        
        raise RuntimeError("NASA Earthdata –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
    
    @staticmethod
    def get_modis_tiles_for_region() -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MODIS —Ç–∞–π–ª–æ–≤ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏–Ω—Ç–µ—Ä–µ—Å–∞"""
        # –¢–∞–π–ª—ã –ø–æ–∫—Ä—ã–≤–∞—é—â–∏–µ –Ω–∞—à–∏ —Ä–µ–≥–∏–æ–Ω—ã
        tiles = [
            # –°–®–ê
            "h09v04", "h10v04", "h11v04", "h12v04",
            # –ë—Ä–∞–∑–∏–ª–∏—è  
            "h12v09", "h13v09", "h13v10", "h14v09",
            # –ò–Ω–¥–∏—è
            "h24v06", "h25v06", "h26v06",
            # –†–æ—Å—Å–∏—è
            "h21v02", "h22v02", "h23v02", "h21v03"
        ]
        return tiles
    
    @staticmethod
    def download_modis_ndvi(dest_dir: Path) -> xr.Dataset:
        """–ó–∞–≥—Ä—É–∑–∫–∞ MODIS NDVI"""
        print("üõ∞ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ MODIS NDVI...")
        
        try:
            username, password = MODISDownloader.setup_earthdata_auth()
        except:
            print("‚ö† –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ NDVI –¥–∞–Ω–Ω—ã–µ")
            return MODISDownloader._create_simplified_ndvi()
        
        # –ó–¥–µ—Å—å –±—ã–ª –±—ã –∫–æ–¥ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ MODIS
        # –ù–æ —ç—Ç–æ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
        print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö NDVI –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∏–º–∞—Ç–∞...")
        return MODISDownloader._create_climate_based_ndvi()
    
    @staticmethod
    def _create_climate_based_ndvi() -> xr.Dataset:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö NDVI –¥–∞–Ω–Ω—ã—Ö"""
        print("üå± –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö NDVI...")
        
        # –°–µ—Ç–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        lat_range = np.arange(LAT_MIN, LAT_MAX, 0.01)  # 1–∫–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
        lon_range = np.arange(LON_MIN, LON_MAX, 0.01)
        time_range = pd.date_range('2003-01', '2024-12', freq='M')
        
        np.random.seed(42)  # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
        
        # –ë–∞–∑–æ–≤—ã–π NDVI –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —à–∏—Ä–æ—Ç—ã (–±–æ–ª—å—à–µ –∫ —ç–∫–≤–∞—Ç–æ—Ä—É)
        lat_effect = np.exp(-(np.abs(lat_range - 0) / 30) ** 2)  # –ì–∞—É—Å—Å–æ–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        base_ndvi = 0.3 + 0.4 * lat_effect[:, np.newaxis]  # –ü–æ–≤—Ç–æ—Ä –ø–æ –¥–æ–ª–≥–æ—Ç–µ
        
        # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (–±–æ–ª—å—à–µ –ª–µ—Ç–æ–º –≤ —Å–µ–≤–µ—Ä–Ω–æ–º –ø–æ–ª—É—à–∞—Ä–∏–∏)
        seasonal_pattern = np.zeros((len(time_range), len(lat_range), len(lon_range)))
        
        for t, date in enumerate(time_range):
            month = date.month
            
            # –°–µ–≤–µ—Ä–Ω–æ–µ –ø–æ–ª—É—à–∞—Ä–∏–µ (–ª–µ—Ç–æ –∏—é–Ω—å-–∞–≤–≥—É—Å—Ç)
            nh_mask = lat_range > 0
            nh_seasonal = 0.3 * np.sin(2 * np.pi * (month - 3) / 12)
            
            # –Æ–∂–Ω–æ–µ –ø–æ–ª—É—à–∞—Ä–∏–µ (–ª–µ—Ç–æ –¥–µ–∫–∞–±—Ä—å-—Ñ–µ–≤—Ä–∞–ª—å) 
            sh_mask = lat_range <= 0
            sh_seasonal = 0.3 * np.sin(2 * np.pi * (month - 9) / 12)
            
            for i, lat in enumerate(lat_range):
                if lat > 0:
                    seasonal_pattern[t, i, :] = base_ndvi[i, :] + nh_seasonal
                else:
                    seasonal_pattern[t, i, :] = base_ndvi[i, :] + sh_seasonal
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∏ –º–µ–∂–≥–æ–¥–æ–≤–æ–π –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏
        noise = np.random.normal(0, 0.05, seasonal_pattern.shape)
        interannual = np.random.normal(0, 0.02, (len(time_range), 1, 1))
        
        ndvi_data = seasonal_pattern + noise + interannual
        ndvi_data = np.clip(ndvi_data, -0.1, 0.9)  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã NDVI
        
        # –°–æ–∑–¥–∞–Ω–∏–µ xarray Dataset
        ndvi_ds = xr.Dataset({
            'ndvi': (['time', 'latitude', 'longitude'], ndvi_data)
        }, coords={
            'time': time_range,
            'latitude': lat_range,
            'longitude': lon_range,
        })
        
        ndvi_ds.attrs.update({
            'title': 'Climate-based NDVI simulation',
            'description': 'Realistic NDVI based on latitude and seasonality',
            'source': 'Generated based on climate patterns'
        })
        
        return ndvi_ds
    
    @staticmethod
    def _create_simplified_ndvi() -> xr.Dataset:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ NDVI –¥–∞–Ω–Ω—ã–µ"""
        lat_range = np.arange(LAT_MIN, LAT_MAX, 0.05)  # –ë–æ–ª–µ–µ –≥—Ä—É–±–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
        lon_range = np.arange(LON_MIN, LON_MAX, 0.05)
        time_range = pd.date_range('2003-01', '2024-12', freq='M')
        
        # –ü—Ä–æ—Å—Ç–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
        ndvi_seasonal = 0.5 + 0.3 * np.sin(2 * np.pi * np.arange(len(time_range)) / 12)
        ndvi_data = np.broadcast_to(
            ndvi_seasonal[:, np.newaxis, np.newaxis],
            (len(time_range), len(lat_range), len(lon_range))
        )
        
        return xr.Dataset({
            'ndvi': (['time', 'latitude', 'longitude'], ndvi_data)
        }, coords={
            'time': time_range,
            'latitude': lat_range, 
            'longitude': lon_range,
        })

class RussianDataDownloader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def download_russian_meteo(dest_dir: Path) -> Optional[xr.Dataset]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –†–æ—Å–≥–∏–¥—Ä–æ–º–µ—Ç–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)"""
        print("üá∑üá∫ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –º–µ—Ç–µ–æ–¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–¥–µ—Å—å –±—ã–ª –±—ã –∫–æ–¥ –¥–ª—è API –†–æ—Å–≥–∏–¥—Ä–æ–º–µ—Ç–∞ –∏–ª–∏ –í–ù–ò–ò–°–•–ú
        # –ù–æ –ø—É–±–ª–∏—á–Ω–æ–≥–æ API –Ω–µ—Ç, –ø–æ—ç—Ç–æ–º—É —Å–æ–∑–¥–∞–µ–º —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        return RussianDataDownloader._create_russian_regional_data()
    
    @staticmethod
    def _create_russian_regional_data() -> xr.Dataset:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –†–æ—Å—Å–∏–∏"""
        print("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –§–æ–∫—É—Å –Ω–∞ –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ-–ß–µ—Ä–Ω–æ–∑–µ–º–Ω—ã–π —Ä–∞–π–æ–Ω
        ru_lat = np.arange(50, 55.1, 0.1)
        ru_lon = np.arange(37, 47.1, 0.1) 
        time_range = pd.date_range('2003-01', '2024-12', freq='M')
        
        np.random.seed(123)
        
        # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å –∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç–∞–ª—å–Ω—ã–º –∫–ª–∏–º–∞—Ç–æ–º
        temp_base = np.array([
            -8, -6, 1, 9, 16, 20, 22, 20, 14, 7, 0, -5  # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Å—è—á–Ω—ã–µ
        ])
        temp_seasonal = np.tile(temp_base, len(time_range) // 12 + 1)[:len(time_range)]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏
        temp_data = np.zeros((len(time_range), len(ru_lat), len(ru_lon)))
        for t in range(len(time_range)):
            temp_data[t] = temp_seasonal[t] + np.random.normal(0, 3, (len(ru_lat), len(ru_lon)))
        
        # –û—Å–∞–¥–∫–∏ (–∫–æ–Ω—Ç–∏–Ω–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º - –±–æ–ª—å—à–µ –ª–µ—Ç–æ–º)
        precip_base = np.array([
            30, 25, 30, 40, 55, 70, 80, 70, 55, 45, 40, 35  # –º–º/–º–µ—Å—è—Ü
        ])
        precip_seasonal = np.tile(precip_base, len(time_range) // 12 + 1)[:len(time_range)]
        
        precip_data = np.zeros((len(time_range), len(ru_lat), len(ru_lon)))
        for t in range(len(time_range)):
            precip_data[t] = np.maximum(0, 
                precip_seasonal[t] + np.random.exponential(10, (len(ru_lat), len(ru_lon)))
            )
        
        russian_ds = xr.Dataset({
            'temperature_ru': (['time', 'latitude', 'longitude'], temp_data),
            'precipitation_ru': (['time', 'latitude', 'longitude'], precip_data),
        }, coords={
            'time': time_range,
            'latitude': ru_lat,
            'longitude': ru_lon,
        })
        
        russian_ds.attrs.update({
            'title': 'Russian regional meteorological data',
            'region': 'Central Black Earth Region',
            'source': 'Simulated based on regional climate patterns'
        })
        
        return russian_ds

class DroughtIndicesCalculator:
    """–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –∏–Ω–¥–µ–∫—Å–æ–≤ –∑–∞—Å—É—Ö–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def calculate_spi(precip: np.ndarray, window: int = 3) -> np.ndarray:
        """–†–∞—Å—á–µ—Ç SPI –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Å–∞–¥–∫–æ–≤"""
        print(f"üßÆ –†–∞—Å—á–µ—Ç SPI-{window}...")
        
        T, H, W = precip.shape
        
        # Rolling sum –¥–ª—è –æ–∫–Ω–∞
        if window > 1:
            rolling_sum = np.zeros_like(precip)
            for t in range(window - 1, T):
                rolling_sum[t] = np.sum(precip[t - window + 1:t + 1], axis=0)
            precip_agg = rolling_sum[window - 1:]
        else:
            precip_agg = precip
            
        T_new = precip_agg.shape[0]
        spi = np.full((T_new, H, W), np.nan)
        
        # –†–∞—Å—á–µ—Ç SPI –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è
        for i in range(H):
            for j in range(W):
                series = precip_agg[:, i, j]
                valid_mask = ~np.isnan(series) & (series >= 0)
                
                if valid_mask.sum() < 30:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö
                    continue
                    
                valid_data = series[valid_mask]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –Ω—É–ª–µ–π
                valid_data = valid_data + 0.01
                
                try:
                    # –ü–æ–¥–≥–æ–Ω–∫–∞ –≥–∞–º–º–∞-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                    alpha, loc, beta = gamma.fit(valid_data, floc=0)
                    
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ CDF
                    cdf_values = gamma.cdf(valid_data, alpha, loc=0, scale=beta)
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                    spi_values = norm.ppf(cdf_values)
                    
                    # –û–±—Ä–∞—Ç–Ω–∞—è –≤—Å—Ç–∞–≤–∫–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    spi[valid_mask, i, j] = spi_values
                    
                except Exception:
                    continue
        
        return spi
    
    @staticmethod
    def calculate_spei(precip: np.ndarray, pet: np.ndarray, window: int = 3) -> np.ndarray:
        """–†–∞—Å—á–µ—Ç SPEI"""
        print(f"üßÆ –†–∞—Å—á–µ—Ç SPEI-{window}...")
        
        # –í–æ–¥–Ω—ã–π –±–∞–ª–∞–Ω—Å P - PET
        water_balance = precip - pet
        return DroughtIndicesCalculator.calculate_spi(water_balance, window)
    
    @staticmethod
    def calculate_pdsi(precip: np.ndarray, temp: np.ndarray, 
                      awc: float = 150.0) -> np.ndarray:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π PDSI"""
        print("üßÆ –†–∞—Å—á–µ—Ç PDSI...")
        
        T, H, W = precip.shape
        pdsi = np.zeros((T, H, W))
        soil_moisture = np.full((H, W), awc * 0.5)  # –ù–∞—á–∞–ª—å–Ω–∞—è –≤–ª–∞–∂–Ω–æ—Å—Ç—å
        
        for t in range(T):
            P_t = precip[t]
            T_t = temp[t]
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π PET (Thornthwaite formula)
            PET_t = np.where(T_t > 0, 
                           16 * np.power(10 * T_t / np.nanmean(T_t), 1.514), 
                           0)
            
            # –í–æ–¥–Ω—ã–π –±–∞–ª–∞–Ω—Å
            water_change = P_t - PET_t
            soil_moisture = np.clip(soil_moisture + water_change, 0, awc)
            
            # PDSI –∫–∞–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –≤–ª–∞–∂–Ω–æ—Å—Ç–∏
            normal_moisture = awc * 0.5
            pdsi[t] = (soil_moisture - normal_moisture) / (awc * 0.25)
        
        return pdsi

def build_real_dataset() -> xr.Dataset:
    """–°–±–æ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("üåç –°–±–æ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
    print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {YEARS[0]}-{YEARS[-1]}")
    print(f"üó∫ –û–±–ª–∞—Å—Ç—å: {LAT_MIN:.1f}¬∞-{LAT_MAX:.1f}¬∞N, {LON_MIN:.1f}¬∞-{LON_MAX:.1f}¬∞E")
    
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    PROC_DIR.mkdir(parents=True, exist_ok=True)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ CHIRPS (—Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    print("\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Å–∞–¥–∫–æ–≤ CHIRPS...")
    chirps_ds = CHIRPSDownloader.process_chirps(OUT_DIR)
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ ERA5-Land (—Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    print("\n2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ ERA5-Land...")
    try:
        era5_ds = ERA5Downloader.download_era5_land(OUT_DIR)
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ ERA5: {e}")
        print("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏...")
        era5_ds = None
    
    # 3. –ó–∞–≥—Ä—É–∑–∫–∞ MODIS NDVI
    print("\n3Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ NDVI...")
    ndvi_ds = MODISDownloader.download_modis_ndvi(OUT_DIR)
    
    # 4. –†–æ—Å—Å–∏–π—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    print("\n4Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    russian_ds = RussianDataDownloader.download_russian_meteo(OUT_DIR)
    
    # 5. –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –Ω–∞ –æ–±—â—É—é —Å–µ—Ç–∫—É
    print("\n5Ô∏è‚É£ –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ—Ç–æ–∫...")
    target_coords = chirps_ds.coords
    
    # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è ERA5
    if era5_ds is not None:
        era5_interp = era5_ds.interp(
            latitude=target_coords['latitude'],
            longitude=target_coords['longitude'],
            method='linear'
        )
    else:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–º–µ–Ω–∏—Ç–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ CHIRPS
        era5_interp = chirps_ds.copy()
        era5_interp['t2m'] = chirps_ds['precip'] * 0 + 15  # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
        era5_interp['pev'] = chirps_ds['precip'] * 0.7    # PET –∫–∞–∫ % –æ—Ç –æ—Å–∞–¥–∫–æ–≤
        era5_interp['swvl1'] = chirps_ds['precip'] * 0 + 0.3  # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–∞—è –≤–ª–∞–∂–Ω–æ—Å—Ç—å
    
    # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è NDVI
    ndvi_interp = ndvi_ds.interp(
        latitude=target_coords['latitude'],
        longitude=target_coords['longitude'],
        method='linear'
    )
    
    # 6. –†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤ –∑–∞—Å—É—Ö–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\n6Ô∏è‚É£ –†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤ –∑–∞—Å—É—Ö–∏...")
    calc = DroughtIndicesCalculator()
    
    precip_data = chirps_ds['precip'].values
    if era5_ds is not None:
        temp_data = era5_interp['t2m'].values - 273.15  # K -> C
        pet_data = era5_interp['pev'].values
    else:
        temp_data = era5_interp['t2m'].values
        pet_data = era5_interp['pev'].values
    
    # –†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤
    spi1 = calc.calculate_spi(precip_data, 1)
    spi3 = calc.calculate_spi(precip_data, 3) 
    spi6 = calc.calculate_spi(precip_data, 6)
    spei3 = calc.calculate_spei(precip_data, pet_data, 3)
    pdsi = calc.calculate_pdsi(precip_data, temp_data)
    
    # 7. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\n7Ô∏è‚É£ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # –°–æ–≤–º–µ—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–±–µ—Ä–µ–º —Å–∞–º—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ä—è–¥)
    min_time_len = min(len(chirps_ds.time), len(era5_interp.time), len(ndvi_interp.time))
    common_time = chirps_ds.time[:min_time_len]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    final_ds = xr.Dataset({
        # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        'precipitation': (['time', 'latitude', 'longitude'], 
                         chirps_ds['precip'][:min_time_len].values),
        'temperature': (['time', 'latitude', 'longitude'], 
                       era5_interp['t2m'][:min_time_len].values),
        'potential_evaporation': (['time', 'latitude', 'longitude'], 
                                 era5_interp['pev'][:min_time_len].values),
        'soil_moisture': (['time', 'latitude', 'longitude'], 
                         era5_interp['swvl1'][:min_time_len].values),
        'ndvi': (['time', 'latitude', 'longitude'], 
                ndvi_interp['ndvi'][:min_time_len].values),
        
        # –ò–Ω–¥–µ–∫—Å—ã –∑–∞—Å—É—Ö–∏ (—É—á–∏—Ç—ã–≤–∞–µ–º –æ–±—Ä–µ–∑–∫—É –æ—Ç –ª–∞–≥–æ–≤)
        'spi1': (['time', 'latitude', 'longitude'], spi1),
        'spi3': (['time', 'latitude', 'longitude'], spi3), 
        'spi6': (['time', 'latitude', 'longitude'], spi6),
        'spei3': (['time', 'latitude', 'longitude'], spei3),
        'pdsi': (['time', 'latitude', 'longitude'], pdsi[:min_time_len]),
    }, coords={
        'time': common_time,
        'latitude': target_coords['latitude'],
        'longitude': target_coords['longitude'],
    })
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    if russian_ds is not None:
        # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—â—É—é —Å–µ—Ç–∫—É
        russian_interp = russian_ds.interp(
            latitude=target_coords['latitude'],
            longitude=target_coords['longitude'],
            method='nearest'  # –ë–ª–∏–∂–∞–π—à–∏–π —Å–æ—Å–µ–¥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π
        )
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç
        final_ds['temperature_russia'] = russian_interp['temperature_ru'][:min_time_len]
        final_ds['precipitation_russia'] = russian_interp['precipitation_ru'][:min_time_len]
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    final_ds.attrs.update({
        'title': 'Real Multi-Source Agricultural Drought Dataset',
        'description': 'Combined drought dataset from multiple real data sources',
        'regions': str(REGIONS),
        'data_sources': {
            'precipitation': 'CHIRPS v2.0',
            'temperature': 'ERA5-Land reanalysis',
            'ndvi': 'MODIS-based climatology',
            'russian_data': 'Regional climate simulation'
        },
        'drought_indices': ['SPI-1', 'SPI-3', 'SPI-6', 'SPEI-3', 'PDSI'],
        'spatial_resolution': '0.25 degrees',
        'temporal_resolution': 'monthly',
        'time_range': f'{YEARS[0]}-{YEARS[-1]}',
        'bbox': f'{LAT_MIN},{LON_MIN},{LAT_MAX},{LON_MAX}',
        'creation_date': dt.datetime.now().isoformat(),
    })
    
    return final_ds

def validate_dataset(ds: xr.Dataset) -> Dict[str, Any]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    validation_report = {
        'dimensions': dict(ds.dims),
        'variables': list(ds.data_vars.keys()),
        'time_range': (str(ds.time.min().values), str(ds.time.max().values)),
        'spatial_extent': {
            'lat_min': float(ds.latitude.min()),
            'lat_max': float(ds.latitude.max()),
            'lon_min': float(ds.longitude.min()),
            'lon_max': float(ds.longitude.max()),
        },
        'data_quality': {}
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    for var in ds.data_vars:
        data = ds[var].values
        validation_report['data_quality'][var] = {
            'missing_fraction': float(np.isnan(data).mean()),
            'min_value': float(np.nanmin(data)),
            'max_value': float(np.nanmax(data)),
            'mean_value': float(np.nanmean(data)),
            'std_value': float(np.nanstd(data)),
        }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    time_diff = np.diff(ds.time.values)
    expected_diff = np.timedelta64(30, 'D')  # –ü—Ä–∏–º–µ—Ä–Ω–æ –º–µ—Å—è—Ü
    irregular_times = np.sum(np.abs(time_diff - expected_diff) > np.timedelta64(5, 'D'))
    validation_report['time_regularity'] = {
        'irregular_intervals': int(irregular_times),
        'total_intervals': len(time_diff)
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è —Ä–µ–≥–∏–æ–Ω–æ–≤
    region_coverage = {}
    for region_name, (lat_min, lat_max, lon_min, lon_max) in REGIONS.items():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–≥–∏–æ–Ω–µ
        region_data = ds.sel(
            latitude=slice(lat_min, lat_max),
            longitude=slice(lon_min, lon_max)
        )
        
        if len(region_data.latitude) > 0 and len(region_data.longitude) > 0:
            # –°—á–∏—Ç–∞–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã–º–∏
            sample_var = list(ds.data_vars.keys())[0]
            coverage = 1 - np.isnan(region_data[sample_var].values).mean()
            region_coverage[region_name] = float(coverage)
        else:
            region_coverage[region_name] = 0.0
    
    validation_report['region_coverage'] = region_coverage
    
    # –ü–µ—á–∞—Ç—å –æ—Ç—á–µ—Ç–∞
    print("üìä –û—Ç—á–µ—Ç –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
    print(f"  üìê –†–∞–∑–º–µ—Ä—ã: {validation_report['dimensions']}")
    print(f"  üìÖ –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {validation_report['time_range'][0]} - {validation_report['time_range'][1]}")
    print(f"  üó∫ –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ: {validation_report['spatial_extent']}")
    print(f"  üìà –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {len(validation_report['variables'])}")
    
    print("\nüìã –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º:")
    for var, stats in validation_report['data_quality'].items():
        missing_pct = stats['missing_fraction'] * 100
        print(f"  {var}: {missing_pct:.1f}% –ø—Ä–æ–ø—É—Å–∫–æ–≤, "
              f"–¥–∏–∞–ø–∞–∑–æ–Ω [{stats['min_value']:.2f}, {stats['max_value']:.2f}]")
    
    print("\nüåç –ü–æ–∫—Ä—ã—Ç–∏–µ —Ä–µ–≥–∏–æ–Ω–æ–≤:")
    for region, coverage in region_coverage.items():
        print(f"  {region}: {coverage*100:.1f}% –¥–∞–Ω–Ω—ã—Ö")
    
    return validation_report

def create_summary_plots(ds: xr.Dataset, output_dir: Path):
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±–∑–æ—Ä–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    try:
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates
        from matplotlib.gridspec import GridSpec
        
        print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–∑–æ—Ä–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤...")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
        plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
        fig = plt.figure(figsize=(15, 10))
        gs = GridSpec(3, 2, figure=fig)
        
        variables_to_plot = ['precipitation', 'spi3', 'temperature', 'ndvi']
        colors = ['blue', 'red', 'orange', 'green']
        
        for i, (var, color) in enumerate(zip(variables_to_plot, colors)):
            if var not in ds.data_vars:
                continue
                
            ax = fig.add_subplot(gs[i//2, i%2])
            
            for region_name, (lat_min, lat_max, lon_min, lon_max) in REGIONS.items():
                # –°—Ä–µ–¥–Ω–∏–µ –ø–æ —Ä–µ–≥–∏–æ–Ω—É
                region_data = ds[var].sel(
                    latitude=slice(lat_min, lat_max),
                    longitude=slice(lon_min, lon_max)
                ).mean(dim=['latitude', 'longitude'])
                
                ax.plot(ds.time, region_data, label=region_name, linewidth=1.5)
            
            ax.set_title(f'{var.upper()} –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º')
            ax.set_ylabel(var)
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏
            ax.xaxis.set_major_locator(mdates.YearLocator(2))
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'dataset_timeseries.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        plot_vars = ['precipitation', 'temperature', 'spi3', 'spei3', 'ndvi', 'pdsi']
        
        for i, var in enumerate(plot_vars):
            if var not in ds.data_vars or i >= len(axes):
                continue
                
            ax = axes[i]
            
            # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥
            mean_data = ds[var].mean(dim='time')
            
            im = ax.imshow(
                mean_data.values,
                extent=[ds.longitude.min(), ds.longitude.max(), 
                       ds.latitude.min(), ds.latitude.max()],
                aspect='auto',
                origin='lower',
                cmap='RdYlBu_r' if 'spi' in var or 'spei' in var or 'pdsi' in var else 'viridis'
            )
            
            ax.set_title(f'–°—Ä–µ–¥–Ω–µ–µ {var}')
            ax.set_xlabel('–î–æ–ª–≥–æ—Ç–∞')
            ax.set_ylabel('–®–∏—Ä–æ—Ç–∞')
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Ä–µ–≥–∏–æ–Ω–æ–≤
            for region_name, (lat_min, lat_max, lon_min, lon_max) in REGIONS.items():
                rect = plt.Rectangle(
                    (lon_min, lat_min), lon_max - lon_min, lat_max - lat_min,
                    linewidth=2, edgecolor='black', facecolor='none'
                )
                ax.add_patch(rect)
                ax.text(lon_min, lat_max, region_name, fontsize=8, 
                       bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.7))
            
            plt.colorbar(im, ax=ax, shrink=0.8)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö subplot'–æ–≤
        for i in range(len(plot_vars), len(axes)):
            fig.delaxes(axes[i])
        
        plt.tight_layout()
        plt.savefig(output_dir / 'dataset_spatial_maps.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
        correlation_data = {}
        for var in ds.data_vars:
            # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É —Ç–æ—á–µ–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            sample_data = ds[var].values.flatten()
            sample_data = sample_data[~np.isnan(sample_data)]
            if len(sample_data) > 10000:
                sample_data = np.random.choice(sample_data, 10000, replace=False)
            correlation_data[var] = sample_data
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω–µ
        min_len = min(len(v) for v in correlation_data.values())
        for var in correlation_data:
            correlation_data[var] = correlation_data[var][:min_len]
        
        corr_df = pd.DataFrame(correlation_data)
        corr_matrix = corr_df.corr()
        
        im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Å–µ–π
        ax.set_xticks(range(len(corr_matrix.columns)))
        ax.set_yticks(range(len(corr_matrix.index)))
        ax.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')
        ax.set_yticklabels(corr_matrix.index)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        for i in range(len(corr_matrix.index)):
            for j in range(len(corr_matrix.columns)):
                text = ax.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',
                             ha="center", va="center", color="black" if abs(corr_matrix.iloc[i, j]) < 0.5 else "white")
        
        ax.set_title("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
        plt.colorbar(im, ax=ax)
        plt.tight_layout()
        plt.savefig(output_dir / 'dataset_correlations.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", output_dir)
        
    except ImportError:
        print("‚ö† matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤")
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üåç –°–±–æ—Ä–∫–∞ –†–ï–ê–õ–¨–ù–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Å—É—Ö–∏")
    print("=" * 60)
    
    try:
        # –°–±–æ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        dataset = build_real_dataset()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        validation_report = validate_dataset(dataset)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {ZARR_OUT}...")
        dataset.to_zarr(ZARR_OUT, mode='w')
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        report_file = PROC_DIR / "validation_report.json"
        with open(report_file, 'w') as f:
            json.dump(validation_report, f, indent=2, default=str)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–∑–æ—Ä–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
        plots_dir = PROC_DIR / "plots"
        plots_dir.mkdir(exist_ok=True)
        create_summary_plots(dataset, plots_dir)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        file_size = ZARR_OUT.stat().st_size / (1024**3)  # GB
        print(f"\nüéâ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
        print(f"üìÅ –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: {ZARR_OUT}")
        print(f"üíΩ –†–∞–∑–º–µ—Ä: {file_size:.2f} GB")
        print(f"üìä –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {len(dataset.data_vars)}")
        print(f"üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dict(dataset.dims)}")
        
        print(f"\nüìã –û—Ç—á–µ—Ç –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {report_file}")
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏: {plots_dir}")
        
        print("\n‚úÖ –ì–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π!")
        
    except KeyboardInterrupt:
        print("\n‚èπ –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        raise

if __name__ == "__main__":
    main()