"""
PyTorch Lightning DataModule –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∑–∞—Å—É—Ö–∏

–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
- –ó–∞–≥—Ä—É–∑–∫—É –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ Zarr
- –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
- DataLoader'—ã –¥–ª—è PyTorch Lightning
- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: from src.data_pipeline.drought_datamodule import DroughtDataModule
"""

import numpy as np
import pandas as pd
import xarray as xr
import torch
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from typing import Dict, List, Tuple, Optional, Union
from pathlib import Path
import warnings

warnings.filterwarnings("ignore")

class DroughtDataset(Dataset):
    """Dataset –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∑–∞—Å—É—Ö–∏"""
    
    def __init__(self, 
                 data: xr.Dataset,
                 input_vars: List[str],
                 target_var: str,
                 sequence_length: int = 12,
                 prediction_horizon: int = 3,
                 spatial_subset: Optional[Dict] = None,
                 transform: Optional[callable] = None,
                 augment: bool = False):
        
        self.data = data
        self.input_vars = input_vars
        self.target_var = target_var
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.transform = transform
        self.augment = augment
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞
        if spatial_subset:
            self.data = self.data.sel(
                latitude=slice(spatial_subset['lat_min'], spatial_subset['lat_max']),
                longitude=slice(spatial_subset['lon_min'], spatial_subset['lon_max'])
            )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        available_vars = list(self.data.data_vars.keys())
        missing_input_vars = [var for var in self.input_vars if var not in available_vars]
        if missing_input_vars:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤—Ö–æ–¥–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {missing_input_vars}")
        
        if self.target_var not in available_vars:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {self.target_var}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        self._prepare_data()
    
    def _prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {len(self.input_vars)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, "
              f"–æ–∫–Ω–æ={self.sequence_length}, –≥–æ—Ä–∏–∑–æ–Ω—Ç={self.prediction_horizon}")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        input_arrays = []
        for var in self.input_vars:
            input_arrays.append(self.data[var].values)
        
        self.input_data = np.stack(input_arrays, axis=1)  # (time, vars, lat, lon)
        self.target_data = self.data[self.target_var].values  # (time, lat, lon)
        
        # –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        self.n_time, self.n_vars, self.n_lat, self.n_lon = self.input_data.shape
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        self.valid_indices = []
        
        for t in range(self.sequence_length, self.n_time - self.prediction_horizon + 1):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –æ–∫–Ω–µ
            input_window = self.input_data[t - self.sequence_length:t]
            target_window = self.target_data[t:t + self.prediction_horizon]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN
            if not (np.isnan(input_window).any() or np.isnan(target_window).any()):
                self.valid_indices.append(t)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.valid_indices)} –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        if len(self.valid_indices) == 0:
            raise ValueError("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö")
    
    def __len__(self) -> int:
        return len(self.valid_indices)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        t = self.valid_indices[idx]
        
        # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        input_sequence = self.input_data[t - self.sequence_length:t]  # (seq_len, vars, lat, lon)
        
        # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        target_sequence = self.target_data[t:t + self.prediction_horizon]  # (horizon, lat, lon)
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if self.transform:
            input_sequence = self.transform(input_sequence)
            target_sequence = self.transform(target_sequence)
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
        if self.augment:
            input_sequence, target_sequence = self._apply_augmentation(
                input_sequence, target_sequence
            )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        input_tensor = torch.FloatTensor(input_sequence)
        target_tensor = torch.FloatTensor(target_sequence)
        
        return input_tensor, target_tensor
    
    def _apply_augmentation(self, inputs: np.ndarray, targets: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞
        if np.random.random() < 0.3:
            noise_std = 0.01
            inputs = inputs + np.random.normal(0, noise_std, inputs.shape)
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Å–¥–≤–∏–≥ (jitter)
        if np.random.random() < 0.2:
            shift = np.random.randint(-2, 3)
            if shift != 0:
                inputs = np.roll(inputs, shift, axis=0)
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ
        if np.random.random() < 0.3:
            if np.random.random() < 0.5:
                inputs = np.flip(inputs, axis=-2)  # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –ø–æ —à–∏—Ä–æ—Ç–µ
                targets = np.flip(targets, axis=-2)
            if np.random.random() < 0.5:
                inputs = np.flip(inputs, axis=-1)  # –û—Ç—Ä–∞–∂–µ–Ω–∏–µ –ø–æ –¥–æ–ª–≥–æ—Ç–µ
                targets = np.flip(targets, axis=-1)
        
        return inputs, targets
    
    def get_stats(self) -> Dict[str, Dict[str, float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        stats = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ö–æ–¥–Ω—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
        for i, var in enumerate(self.input_vars):
            var_data = self.input_data[:, i, :, :]
            valid_data = var_data[~np.isnan(var_data)]
            
            if len(valid_data) > 0:
                stats[var] = {
                    'mean': float(np.mean(valid_data)),
                    'std': float(np.std(valid_data)),
                    'min': float(np.min(valid_data)),
                    'max': float(np.max(valid_data))
                }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        target_valid = self.target_data[~np.isnan(self.target_data)]
        if len(target_valid) > 0:
            stats[self.target_var] = {
                'mean': float(np.mean(target_valid)),
                'std': float(np.std(target_valid)),
                'min': float(np.min(target_valid)),
                'max': float(np.max(target_valid))
            }
        
        return stats

class DroughtDataModule(pl.LightningDataModule):
    """PyTorch Lightning DataModule –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∑–∞—Å—É—Ö–∏"""
    
    def __init__(self,
                 data_path: str,
                 input_vars: List[str],
                 target_var: str,
                 sequence_length: int = 12,
                 prediction_horizon: int = 3,
                 train_years: Tuple[int, int] = (2003, 2016),
                 val_years: Tuple[int, int] = (2017, 2019),
                 test_years: Tuple[int, int] = (2020, 2024),
                 batch_size: int = 32,
                 num_workers: int = 4,
                 spatial_subset: Optional[Dict] = None,
                 normalize: bool = True,
                 augment_train: bool = False):
        
        super().__init__()
        
        self.data_path = data_path
        self.input_vars = input_vars
        self.target_var = target_var
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.train_years = train_years
        self.val_years = val_years
        self.test_years = test_years
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.spatial_subset = spatial_subset
        self.normalize = normalize
        self.augment_train = augment_train
        
        # –ë—É–¥—É—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤ setup()
        self.data = None
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None
        self.normalizer = None
        
    def prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
        if not Path(self.data_path).exists():
            raise FileNotFoundError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.data_path}")
        
        print(f"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {self.data_path}")
    
    def setup(self, stage: Optional[str] = None):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞—Å—É—Ö–∏...")
        self.data = xr.open_zarr(self.data_path)
        
        print(f"üìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: {dict(self.data.dims)}")
        print(f"üìã –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {list(self.data.data_vars.keys())}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
        if self.normalize:
            self._setup_normalization()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if stage == "fit" or stage is None:
            # –û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
            train_data = self._filter_by_years(self.data, self.train_years)
            self.train_dataset = DroughtDataset(
                data=train_data,
                input_vars=self.input_vars,
                target_var=self.target_var,
                sequence_length=self.sequence_length,
                prediction_horizon=self.prediction_horizon,
                spatial_subset=self.spatial_subset,
                transform=self._get_transform(),
                augment=self.augment_train
            )
            
            # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            val_data = self._filter_by_years(self.data, self.val_years)
            self.val_dataset = DroughtDataset(
                data=val_data,
                input_vars=self.input_vars,
                target_var=self.target_var,
                sequence_length=self.sequence_length,
                prediction_horizon=self.prediction_horizon,
                spatial_subset=self.spatial_subset,
                transform=self._get_transform(),
                augment=False
            )
        
        if stage == "test" or stage is None:
            # –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            test_data = self._filter_by_years(self.data, self.test_years)
            self.test_dataset = DroughtDataset(
                data=test_data,
                input_vars=self.input_vars,
                target_var=self.target_var,
                sequence_length=self.sequence_length,
                prediction_horizon=self.prediction_horizon,
                spatial_subset=self.spatial_subset,
                transform=self._get_transform(),
                augment=False
            )
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self._print_dataset_stats()
    
    def _filter_by_years(self, data: xr.Dataset, years: Tuple[int, int]) -> xr.Dataset:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –≥–æ–¥–∞–º"""
        start_year, end_year = years
        time_mask = (data.time.dt.year >= start_year) & (data.time.dt.year <= end_year)
        return data.sel(time=time_mask)
    
    def _setup_normalization(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –æ–±—É—á–∞—é—â–∏–º –¥–∞–Ω–Ω—ã–º
        train_data = self._filter_by_years(self.data, self.train_years)
        
        self.normalizer = {}
        
        # –î–ª—è –∫–∞–∂–¥–æ–π –≤—Ö–æ–¥–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        for var in self.input_vars:
            var_data = train_data[var].values
            valid_data = var_data[~np.isnan(var_data)]
            
            if len(valid_data) > 0:
                self.normalizer[var] = {
                    'mean': float(np.mean(valid_data)),
                    'std': float(np.std(valid_data))
                }
            else:
                self.normalizer[var] = {'mean': 0.0, 'std': 1.0}
        
        # –î–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        target_data = train_data[self.target_var].values
        valid_target = target_data[~np.isnan(target_data)]
        
        if len(valid_target) > 0:
            self.normalizer[self.target_var] = {
                'mean': float(np.mean(valid_target)),
                'std': float(np.std(valid_target))
            }
        else:
            self.normalizer[self.target_var] = {'mean': 0.0, 'std': 1.0}
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∫ –¥–∞–Ω–Ω—ã–º
        self._apply_normalization()
    
    def _apply_normalization(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∫ –¥–∞–Ω–Ω—ã–º"""
        normalized_data = {}
        
        for var in self.data.data_vars:
            if var in self.normalizer:
                mean = self.normalizer[var]['mean']
                std = self.normalizer[var]['std']
                normalized_data[var] = (self.data[var] - mean) / (std + 1e-8)
            else:
                normalized_data[var] = self.data[var]
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π Dataset —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        self.data = xr.Dataset(normalized_data, coords=self.data.coords, attrs=self.data.attrs)
    
    def _get_transform(self) -> Optional[callable]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        return None
    
    def _print_dataset_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:")
        
        if self.train_dataset:
            print(f"  üèãÔ∏è –û–±—É—á–µ–Ω–∏–µ: {len(self.train_dataset)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        if self.val_dataset:
            print(f"  üîç –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(self.val_dataset)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        if self.test_dataset:
            print(f"  üß™ –¢–µ—Å—Ç: {len(self.test_dataset)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if self.train_dataset:
            stats = self.train_dataset.get_stats()
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä):")
            for var, var_stats in stats.items():
                print(f"  {var}: mean={var_stats['mean']:.3f}, std={var_stats['std']:.3f}")
    
    def train_dataloader(self) -> DataLoader:
        """DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True if self.num_workers > 0 else False,
            drop_last=True
        )
    
    def val_dataloader(self) -> DataLoader:
        """DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True if self.num_workers > 0 else False
        )
    
    def test_dataloader(self) -> DataLoader:
        """DataLoader –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
            persistent_workers=True if self.num_workers > 0 else False
        )
    
    def predict_dataloader(self) -> DataLoader:
        """DataLoader –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        return self.test_dataloader()
    
    def get_sample_batch(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –±–∞—Ç—á–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        if self.train_dataset is None:
            raise ValueError("–î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã. –í—ã–∑–æ–≤–∏—Ç–µ setup() —Å–Ω–∞—á–∞–ª–∞.")
        
        sample_loader = DataLoader(self.train_dataset, batch_size=2, shuffle=False)
        return next(iter(sample_loader))
    
    def denormalize(self, data: torch.Tensor, var_name: str) -> torch.Tensor:
        """–î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        if self.normalizer and var_name in self.normalizer:
            mean = self.normalizer[var_name]['mean']
            std = self.normalizer[var_name]['std']
            return data * std + mean
        return data
    
    def get_coords(self) -> Dict[str, np.ndarray]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"""
        if self.data is None:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        
        coords = {}
        if self.spatial_subset:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
            data_subset = self.data.sel(
                latitude=slice(self.spatial_subset['lat_min'], self.spatial_subset['lat_max']),
                longitude=slice(self.spatial_subset['lon_min'], self.spatial_subset['lon_max'])
            )
            coords['latitude'] = data_subset.latitude.values
            coords['longitude'] = data_subset.longitude.values
        else:
            coords['latitude'] = self.data.latitude.values
            coords['longitude'] = self.data.longitude.values
        
        coords['time'] = self.data.time.values
        return coords

# –§—É–Ω–∫—Ü–∏–∏-—É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å DataModule
def create_drought_datamodule(config: Dict) -> DroughtDataModule:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DroughtDataModule –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    data_config = config.get('data', {})
    training_config = config.get('training', {})
    
    # –°–æ–∑–¥–∞–µ–º DataModule
    dm = DroughtDataModule(
        data_path=data_config.get('paths', {}).get('output_zarr', 'data/processed/real_agro_cube.zarr'),
        input_vars=data_config.get('input_variables', ['precipitation', 'temperature', 'ndvi']),
        target_var=data_config.get('target_variable', 'spi3'),
        sequence_length=training_config.get('sequence_length', 12),
        prediction_horizon=training_config.get('prediction_horizon', 3),
        train_years=tuple(training_config.get('train_years', [2003, 2016])),
        val_years=tuple(training_config.get('val_years', [2017, 2019])),
        test_years=tuple(training_config.get('test_years', [2020, 2024])),
        batch_size=training_config.get('batch_size', 32),
        num_workers=config.get('compute', {}).get('num_workers', 4),
        spatial_subset=config.get('spatial_subset'),
        normalize=data_config.get('normalize', True),
        augment_train=training_config.get('data_augmentation', {}).get('enabled', False)
    )
    
    return dm

def get_datamodule_stats(dm: DroughtDataModule) -> Dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ DataModule"""
    
    if dm.train_dataset is None:
        raise ValueError("DataModule –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –í—ã–∑–æ–≤–∏—Ç–µ dm.setup() —Å–Ω–∞—á–∞–ª–∞.")
    
    stats = {
        'train_size': len(dm.train_dataset),
        'val_size': len(dm.val_dataset) if dm.val_dataset else 0,
        'test_size': len(dm.test_dataset) if dm.test_dataset else 0,
        'input_vars': dm.input_vars,
        'target_var': dm.target_var,
        'sequence_length': dm.sequence_length,
        'prediction_horizon': dm.prediction_horizon,
        'batch_size': dm.batch_size,
        'spatial_dims': None,
        'temporal_dims': None
    }
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—Ä–∏–º–µ—Ä–∞
    try:
        sample_input, sample_target = dm.get_sample_batch()
        stats['input_shape'] = list(sample_input.shape)
        stats['target_shape'] = list(sample_target.shape)
        stats['spatial_dims'] = sample_input.shape[-2:]  # (lat, lon)
        stats['temporal_dims'] = sample_input.shape[1]   # sequence_length
    except:
        pass
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    if dm.train_dataset:
        stats['variable_stats'] = dm.train_dataset.get_stats()
    
    return stats

def validate_datamodule(dm: DroughtDataModule) -> Dict[str, bool]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è DataModule"""
    
    validation_results = {
        'data_loaded': False,
        'datasets_created': False,
        'dataloaders_work': False,
        'batch_shapes_correct': False,
        'no_nan_in_batches': False
    }
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö
        if dm.data is not None:
            validation_results['data_loaded'] = True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if dm.train_dataset and dm.val_dataset and dm.test_dataset:
            validation_results['datasets_created'] = True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É DataLoader'–æ–≤
        train_loader = dm.train_dataloader()
        val_loader = dm.val_dataloader()
        test_loader = dm.test_dataloader()
        
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –±–∞—Ç—á
        train_batch = next(iter(train_loader))
        val_batch = next(iter(val_loader))
        test_batch = next(iter(test_loader))
        
        validation_results['dataloaders_work'] = True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º—É –±–∞—Ç—á–µ–π
        input_train, target_train = train_batch
        input_val, target_val = val_batch
        input_test, target_test = test_batch
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤
        expected_input_dims = 5  # (batch, time, vars, lat, lon)
        expected_target_dims = 4  # (batch, horizon, lat, lon)
        
        if (len(input_train.shape) == expected_input_dims and 
            len(target_train.shape) == expected_target_dims):
            validation_results['batch_shapes_correct'] = True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ NaN
        if (not torch.isnan(input_train).any() and 
            not torch.isnan(target_train).any() and
            not torch.isnan(input_val).any() and 
            not torch.isnan(target_val).any()):
            validation_results['no_nan_in_batches'] = True
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
    
    return validation_results

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("üì¶ DroughtDataModule –¥–ª—è PyTorch Lightning")
    
    # –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = {
        'data': {
            'paths': {
                'output_zarr': 'data/processed/real_agro_cube.zarr'
            },
            'input_variables': ['precipitation', 'temperature', 'ndvi', 'soil_moisture'],
            'target_variable': 'spi3',
            'normalize': True
        },
        'training': {
            'sequence_length': 12,
            'prediction_horizon': 3,
            'train_years': [2003, 2016],
            'val_years': [2017, 2019],
            'test_years': [2020, 2024],
            'batch_size': 32,
            'data_augmentation': {
                'enabled': False
            }
        },
        'compute': {
            'num_workers': 4
        }
    }
    
    print("‚öôÔ∏è –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    print("\nüí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
    print("# –°–æ–∑–¥–∞–Ω–∏–µ DataModule –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    print("dm = create_drought_datamodule(config)")
    print("dm.setup()")
    print()
    print("# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
    print("stats = get_datamodule_stats(dm)")
    print()
    print("# –í–∞–ª–∏–¥–∞—Ü–∏—è DataModule")
    print("validation = validate_datamodule(dm)")
    print()
    print("# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ PyTorch Lightning")
    print("trainer = pl.Trainer()")
    print("trainer.fit(model, dm)")
    
    print("\n‚úÖ DroughtDataModule –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    
    # –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª–∞—Å—Å—ã
    print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª–∞—Å—Å—ã:")
    print("  - DroughtDataset: –ë–∞–∑–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
    print("  - DroughtDataModule: –û—Å–Ω–æ–≤–Ω–æ–π DataModule")
    
    print("\nüîß –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:")
    print("  - create_drought_datamodule(): –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    print("  - get_datamodule_stats(): –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
    print("  - validate_datamodule(): –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏")