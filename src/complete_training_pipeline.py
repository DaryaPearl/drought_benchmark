"""
–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Å—É—Ö–∏
–í–∫–ª—é—á–∞–µ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ ML –º–æ–¥–µ–ª–∏ –∏ SOTA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

–ó–∞–ø—É—Å–∫:
    python src/complete_training_pipeline.py --model all --experiment drought_prediction_2024
    python src/complete_training_pipeline.py --model earthformer --config configs/earthformer.yaml
    python src/complete_training_pipeline.py --model classical --fast
"""

import argparse
import os
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import yaml
import json
from datetime import datetime
import time

import numpy as np
import pandas as pd
import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
import xarray as xr
from torch.utils.data import DataLoader, Dataset

# –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã)
from src.models.sota_models import get_model, CONFIGS
from src.utils.metrics import DroughtMetrics
from src.utils.visualization import ResultsVisualizer, TrainingVisualizer

warnings.filterwarnings("ignore")

class DroughtDataset(Dataset):
    """PyTorch Dataset –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∑–∞—Å—É—Ö–∏"""
    
    def __init__(self, data: xr.Dataset, 
                 input_vars: List[str],
                 target_var: str,
                 sequence_length: int = 12,
                 prediction_horizon: int = 3,
                 train_years: Tuple[int, int] = (2003, 2016),
                 spatial_subset: Optional[Dict] = None):
        
        self.data = data
        self.input_vars = input_vars
        self.target_var = target_var
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≥–æ–¥–∞–º
        start_year, end_year = train_years
        time_mask = (data.time.dt.year >= start_year) & (data.time.dt.year <= end_year)
        self.data = data.sel(time=time_mask)
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if spatial_subset:
            self.data = self.data.sel(
                latitude=slice(spatial_subset['lat_min'], spatial_subset['lat_max']),
                longitude=slice(spatial_subset['lon_min'], spatial_subset['lon_max'])
            )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        self._prepare_sequences()
    
    def _prepare_sequences(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–æ–∫–Ω–æ={self.sequence_length}, –≥–æ—Ä–∏–∑–æ–Ω—Ç={self.prediction_horizon})")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy
        input_data = []
        for var in self.input_vars:
            if var in self.data.data_vars:
                input_data.append(self.data[var].values)
        
        input_array = np.stack(input_data, axis=1)  # (time, variables, lat, lon)
        target_array = self.data[self.target_var].values  # (time, lat, lon)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        self.sequences = []
        self.targets = []
        
        T = input_array.shape[0]
        for t in range(self.sequence_length, T - self.prediction_horizon):
            # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            seq = input_array[t - self.sequence_length:t]
            
            # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (—á–µ—Ä–µ–∑ prediction_horizon —à–∞–≥–æ–≤)
            target_time_indices = range(t, t + self.prediction_horizon)
            target = target_array[target_time_indices]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
            if not (np.isnan(seq).any() or np.isnan(target).any()):
                self.sequences.append(seq)
                self.targets.append(target)
        
        self.sequences = np.array(self.sequences)
        self.targets = np.array(self.targets)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.sequences)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        print(f"üìê –§–æ—Ä–º–∞ –≤—Ö–æ–¥–∞: {self.sequences.shape}")
        print(f"üìê –§–æ—Ä–º–∞ —Ü–µ–ª–∏: {self.targets.shape}")
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return (
            torch.FloatTensor(self.sequences[idx]),
            torch.FloatTensor(self.targets[idx])
        )

class ClassicalModelsTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π ML"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.models = {
            'linear_regression': LinearRegression(),
            'ridge': Ridge(alpha=config.get('ridge_alpha', 1.0)),
            'lasso': Lasso(alpha=config.get('lasso_alpha', 1.0)),
            'random_forest': RandomForestRegressor(
                n_estimators=config.get('rf_n_estimators', 300),
                max_depth=config.get('rf_max_depth', 20),
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=config.get('gb_n_estimators', 200),
                learning_rate=config.get('gb_learning_rate', 0.1),
                max_depth=config.get('gb_max_depth', 6),
                random_state=42
            ),
            'svr': SVR(
                kernel=config.get('svr_kernel', 'rbf'),
                C=config.get('svr_C', 1.0),
                epsilon=config.get('svr_epsilon', 0.1)
            )
        }
        
        self.scaler = StandardScaler()
        self.results = {}
    
    def prepare_classical_data(self, dataset: DroughtDataset) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π...")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ –ø–ª–æ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        sequences = dataset.sequences  # (samples, time, vars, lat, lon)
        targets = dataset.targets     # (samples, horizon, lat, lon)
        
        # Flatten –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤
        n_samples, seq_len, n_vars, n_lat, n_lon = sequences.shape
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –∫–∞–∂–¥—ã–π –ø–∏–∫—Å–µ–ª—å –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–æ–º
        features = []
        labels = []
        
        for sample_idx in range(n_samples):
            seq = sequences[sample_idx]  # (time, vars, lat, lon)
            target = targets[sample_idx]  # (horizon, lat, lon)
            
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è
            for i in range(n_lat):
                for j in range(n_lon):
                    # –ü—Ä–∏–∑–Ω–∞–∫–∏: –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –≤—Å–µ—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–∏–∫—Å–µ–ª—è
                    pixel_features = seq[:, :, i, j].flatten()  # (time * vars,)
                    
                    # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: —Å—Ä–µ–¥–Ω–∏–π SPI –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç—É –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–∏–∫—Å–µ–ª—è
                    pixel_target = target[:, i, j].mean()  # –°—Ä–µ–¥–Ω–µ–µ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç—É
                    
                    if not (np.isnan(pixel_features).any() or np.isnan(pixel_target)):
                        features.append(pixel_features)
                        labels.append(pixel_target)
        
        X = np.array(features)
        y = np.array(labels)
        
        print(f"üìä –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: X={X.shape}, y={y.shape}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = self.scaler.fit_transform(X)
        
        return X, y
    
    def train_classical_models(self, train_dataset: DroughtDataset, 
                             val_dataset: DroughtDataset) -> Dict[str, Dict]:
        """–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π ML...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train, y_train = self.prepare_classical_data(train_dataset)
        X_val, y_val = self.prepare_classical_data(val_dataset)
        
        results = {}
        
        for model_name, model in self.models.items():
            print(f"\nüîß –û–±—É—á–µ–Ω–∏–µ {model_name}...")
            start_time = time.time()
            
            try:
                # –û–±—É—á–µ–Ω–∏–µ
                model.fit(X_train, y_train)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                y_train_pred = model.predict(X_train)
                y_val_pred = model.predict(X_val)
                
                # –ú–µ—Ç—Ä–∏–∫–∏
                train_metrics = {
                    'mae': mean_absolute_error(y_train, y_train_pred),
                    'rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),
                    'r2': r2_score(y_train, y_train_pred)
                }
                
                val_metrics = {
                    'mae': mean_absolute_error(y_val, y_val_pred),
                    'rmse': np.sqrt(mean_squared_error(y_val, y_val_pred)),
                    'r2': r2_score(y_val, y_val_pred)
                }
                
                training_time = time.time() - start_time
                
                results[model_name] = {
                    'train_metrics': train_metrics,
                    'val_metrics': val_metrics,
                    'training_time': training_time,
                    'model': model
                }
                
                print(f"  ‚úÖ {model_name}: Val MAE={val_metrics['mae']:.4f}, "
                      f"R¬≤={val_metrics['r2']:.4f}, Time={training_time:.1f}s")
                
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {model_name}: {e}")
                results[model_name] = {'error': str(e)}
        
        return results

class SOTAModelsTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –¥–ª—è SOTA deep learning –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.results = {}
    
    def create_data_loaders(self, train_dataset: DroughtDataset,
                           val_dataset: DroughtDataset,
                           test_dataset: DroughtDataset) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤"""
        batch_size = self.config.get('batch_size', 32)
        num_workers = self.config.get('num_workers', 4)
        
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )
        
        return train_loader, val_loader, test_loader
    
    def train_sota_model(self, model_name: str, 
                        train_loader: DataLoader,
                        val_loader: DataLoader,
                        test_loader: DataLoader) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π SOTA –º–æ–¥–µ–ª–∏"""
        print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ {model_name.upper()}...")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model_config = CONFIGS.get(model_name, {})
        model_config.update(self.config.get(f'{model_name}_config', {}))
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model = get_model(model_name, model_config)
            
            # Callbacks
            callbacks = [
                ModelCheckpoint(
                    monitor='val_loss',
                    dirpath=f'checkpoints/{model_name}',
                    filename='{epoch:02d}-{val_loss:.4f}',
                    save_top_k=3,
                    mode='min'
                ),
                EarlyStopping(
                    monitor='val_loss',
                    patience=self.config.get('early_stopping_patience', 15),
                    mode='min'
                ),
                LearningRateMonitor(logging_interval='epoch')
            ]
            
            # Logger
            logger = TensorBoardLogger(
                save_dir='lightning_logs',
                name=model_name,
                version=datetime.now().strftime("%Y%m%d_%H%M%S")
            )
            
            # Trainer
            trainer = pl.Trainer(
                max_epochs=self.config.get('max_epochs', 100),
                accelerator='auto',
                devices='auto',
                callbacks=callbacks,
                logger=logger,
                gradient_clip_val=self.config.get('gradient_clip_val', 1.0),
                accumulate_grad_batches=self.config.get('accumulate_grad_batches', 1),
                precision=self.config.get('precision', 32),
                log_every_n_steps=10,
            )
            
            # –û–±—É—á–µ–Ω–∏–µ
            start_time = time.time()
            trainer.fit(model, train_loader, val_loader)
            training_time = time.time() - start_time
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            test_results = trainer.test(model, test_loader)
            
            # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results = {
                'train_metrics': {
                    'final_train_loss': float(trainer.callback_metrics.get('train_loss', 0)),
                },
                'val_metrics': {
                    'final_val_loss': float(trainer.callback_metrics.get('val_loss', 0)),
                    'best_val_loss': float(callbacks[0].best_model_score),
                },
                'test_metrics': test_results[0] if test_results else {},
                'training_time': training_time,
                'best_model_path': callbacks[0].best_model_path,
                'model_config': model_config
            }
            
            print(f"  ‚úÖ {model_name}: Val Loss={results['val_metrics']['final_val_loss']:.4f}, "
                  f"Time={training_time:.1f}s")
            
            return results
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {model_name}: {e}")
            return {'error': str(e)}

class ExperimentManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.experiment_name = config.get('experiment_name', f'drought_exp_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
        self.results_dir = Path(config.get('results_dir', 'results')) / self.experiment_name
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.all_results = {}
        
    def prepare_datasets(self) -> Tuple[DroughtDataset, DroughtDataset, DroughtDataset]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data_path = self.config.get('data_path', 'data/processed/real_agro_cube.zarr')
        dataset = xr.open_zarr(data_path)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        input_vars = self.config.get('input_variables', ['precipitation', 'temperature', 'ndvi', 'soil_moisture'])
        target_var = self.config.get('target_variable', 'spi3')
        sequence_length = self.config.get('sequence_length', 12)
        prediction_horizon = self.config.get('prediction_horizon', 3)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º
        train_years = self.config.get('train_years', (2003, 2016))
        val_years = self.config.get('val_years', (2017, 2019))
        test_years = self.config.get('test_years', (2020, 2024))
        
        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        spatial_subset = self.config.get('spatial_subset', None)
        if self.config.get('fast_mode', False):
            # –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º - –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö
            spatial_subset = {
                'lat_min': 40, 'lat_max': 50,
                'lon_min': -100, 'lon_max': -80
            }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        train_dataset = DroughtDataset(
            dataset, input_vars, target_var, sequence_length, prediction_horizon,
            train_years, spatial_subset
        )
        
        val_dataset = DroughtDataset(
            dataset, input_vars, target_var, sequence_length, prediction_horizon,
            val_years, spatial_subset
        )
        
        test_dataset = DroughtDataset(
            dataset, input_vars, target_var, sequence_length, prediction_horizon,
            test_years, spatial_subset
        )
        
        return train_dataset, val_dataset, test_dataset
    
    def run_experiment(self, models_to_train: List[str]):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        print(f"üß™ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {self.experiment_name}")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.results_dir}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train_dataset, val_dataset, test_dataset = self.prepare_datasets()
        
        # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏
        if 'classical' in models_to_train or 'all' in models_to_train:
            print("\n" + "="*50)
            print("ü§ñ –ö–õ–ê–°–°–ò–ß–ï–°–ö–ò–ï –ú–û–î–ï–õ–ò –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
            print("="*50)
            
            classical_trainer = ClassicalModelsTrainer(self.config)
            classical_results = classical_trainer.train_classical_models(train_dataset, val_dataset)
            self.all_results['classical'] = classical_results
        
        # SOTA –º–æ–¥–µ–ª–∏
        sota_models = ['earthformer', 'convlstm', 'tft', 'unet']
        sota_to_train = [m for m in models_to_train if m in sota_models] or (sota_models if 'all' in models_to_train else [])
        
        if sota_to_train:
            print("\n" + "="*50)
            print("üöÄ SOTA DEEP LEARNING –ú–û–î–ï–õ–ò")
            print("="*50)
            
            sota_trainer = SOTAModelsTrainer(self.config)
            train_loader, val_loader, test_loader = sota_trainer.create_data_loaders(
                train_dataset, val_dataset, test_dataset
            )
            
            for model_name in sota_to_train:
                if model_name in CONFIGS:
                    results = sota_trainer.train_sota_model(
                        model_name, train_loader, val_loader, test_loader
                    )
                    self.all_results[model_name] = results
                else:
                    print(f"‚ö† –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.save_results()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        self.create_report()
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.create_visualizations()
        
        print(f"\nüéâ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {self.results_dir}")
    
    def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        results_to_save = {}
        for model_name, results in self.all_results.items():
            if isinstance(results, dict):
                # –£–±–∏—Ä–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –º–æ–¥–µ–ª–µ–π –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                clean_results = {}
                for key, value in results.items():
                    if isinstance(value, dict) and 'model' not in key:
                        clean_results[key] = value
                    elif key != 'model':
                        clean_results[key] = str(value) if not isinstance(value, (int, float, str, bool, type(None))) else value
                results_to_save[model_name] = clean_results
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
        results_file = self.results_dir / 'experiment_results.json'
        with open(results_file, 'w') as f:
            json.dump({
                'experiment_name': self.experiment_name,
                'config': self.config,
                'results': results_to_save,
                'timestamp': datetime.now().isoformat()
            }, f, indent=2, default=str)
        
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
    
    def create_report(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        print("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...")
        
        report_file = self.results_dir / 'experiment_report.md'
        
        with open(report_file, 'w') as f:
            f.write(f"# –û—Ç—á–µ—Ç –ø–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É: {self.experiment_name}\n\n")
            f.write(f"**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            f.write("## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n\n")
            f.write("```yaml\n")
            f.write(yaml.dump(self.config, default_flow_style=False))
            f.write("```\n\n")
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º
            f.write("## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π\n\n")
            
            # –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            f.write("### –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\n\n")
            f.write("| –ú–æ–¥–µ–ª—å | Val MAE | Val RMSE | Val R¬≤ | –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å) |\n")
            f.write("|--------|---------|----------|--------|--------------------|\n")
            
            for model_name, results in self.all_results.items():
                if isinstance(results, dict) and 'error' not in results:
                    if model_name == 'classical':
                        # –î–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –±–µ—Ä–µ–º –ª—É—á—à—É—é –ø–æ Val MAE
                        best_classical = min(results.items(), 
                                           key=lambda x: x[1].get('val_metrics', {}).get('mae', float('inf')))
                        best_name, best_results = best_classical
                        val_metrics = best_results.get('val_metrics', {})
                        f.write(f"| {best_name} (best classical) | "
                               f"{val_metrics.get('mae', 'N/A'):.4f} | "
                               f"{val_metrics.get('rmse', 'N/A'):.4f} | "
                               f"{val_metrics.get('r2', 'N/A'):.4f} | "
                               f"{best_results.get('training_time', 'N/A'):.1f} |\n")
                    else:
                        val_metrics = results.get('val_metrics', {})
                        val_loss = val_metrics.get('final_val_loss', 'N/A')
                        training_time = results.get('training_time', 'N/A')
                        f.write(f"| {model_name} | N/A | {val_loss:.4f} | N/A | {training_time:.1f} |\n")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            f.write("\n### –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n")
            
            for model_name, results in self.all_results.items():
                f.write(f"#### {model_name.upper()}\n\n")
                
                if isinstance(results, dict) and 'error' not in results:
                    if model_name == 'classical':
                        for sub_model, sub_results in results.items():
                            f.write(f"**{sub_model}**\n\n")
                            val_metrics = sub_results.get('val_metrics', {})
                            f.write(f"- Validation MAE: {val_metrics.get('mae', 'N/A'):.4f}\n")
                            f.write(f"- Validation RMSE: {val_metrics.get('rmse', 'N/A'):.4f}\n")
                            f.write(f"- Validation R¬≤: {val_metrics.get('r2', 'N/A'):.4f}\n")
                            f.write(f"- Training time: {sub_results.get('training_time', 'N/A'):.1f}s\n\n")
                    else:
                        val_metrics = results.get('val_metrics', {})
                        test_metrics = results.get('test_metrics', {})
                        f.write(f"- Final validation loss: {val_metrics.get('final_val_loss', 'N/A'):.4f}\n")
                        f.write(f"- Best validation loss: {val_metrics.get('best_val_loss', 'N/A'):.4f}\n")
                        f.write(f"- Training time: {results.get('training_time', 'N/A'):.1f}s\n")
                        if test_metrics:
                            f.write(f"- Test metrics: {test_metrics}\n")
                        f.write("\n")
                else:
                    f.write(f"‚ùå –û—à–∏–±–∫–∞: {results.get('error', 'Unknown error')}\n\n")
        
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_file}")
    
    def create_visualizations(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        
        try:
            visualizer = ResultsVisualizer(self.results_dir)
            
            # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            visualizer.plot_model_comparison(self.all_results)
            
            # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –ª–æ–≥–∏)
            visualizer.plot_training_curves(self.results_dir)
            
            print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")
            
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")

def load_config(config_path: str) -> Dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
    if Path(config_path).exists():
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    else:
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return {
            'experiment_name': f'drought_experiment_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'data_path': 'data/processed/real_agro_cube.zarr',
            'input_variables': ['precipitation', 'temperature', 'ndvi', 'soil_moisture'],
            'target_variable': 'spi3',
            'sequence_length': 12,
            'prediction_horizon': 3,
            'train_years': [2003, 2016],
            'val_years': [2017, 2019],
            'test_years': [2020, 2024],
            'batch_size': 32,
            'max_epochs': 50,
            'early_stopping_patience': 10,
            'learning_rate': 0.001,
            'fast_mode': False,
            'results_dir': 'results'
        }

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Å—É—Ö–∏')
    parser.add_argument('--model', type=str, choices=['all', 'classical', 'earthformer', 'convlstm', 'tft', 'unet'],
                       default='all', help='–ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--config', type=str, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--experiment', type=str, help='–ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞')
    parser.add_argument('--fast', action='store_true', help='–ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º (–º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö)')
    parser.add_argument('--data-path', type=str, help='–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = load_config(args.config) if args.config else load_config('config.yaml')
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏
    if args.experiment:
        config['experiment_name'] = args.experiment
    if args.fast:
        config['fast_mode'] = True
        config['max_epochs'] = 10  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.data_path:
        config['data_path'] = args.data_path
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    if args.model == 'all':
        models_to_train = ['classical', 'earthformer', 'convlstm', 'tft', 'unet']
    else:
        models_to_train = [args.model]
    
    print("üåæ –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Å—É—Ö–∏")
    print("=" * 50)
    print(f"üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {config['experiment_name']}")
    print(f"ü§ñ –ú–æ–¥–µ–ª–∏: {', '.join(models_to_train)}")
    print(f"‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º: {'–î–∞' if config.get('fast_mode') else '–ù–µ—Ç'}")
    print("=" * 50)
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiment_manager = ExperimentManager(config)
        
        # –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiment_manager.run_experiment(models_to_train)
        
    except KeyboardInterrupt:
        print("\n‚èπ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {e}")
        raise

if __name__ == "__main__":
    main()